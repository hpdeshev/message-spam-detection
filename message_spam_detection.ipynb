{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MsTTIkXlqgj7"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "orSsMXkL6YkZ"
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from collections.abc import Iterable\n",
    "import itertools\n",
    "from pathlib import Path\n",
    "\n",
    "import datasets\n",
    "import gradio as ui\n",
    "from matplotlib.axes import Axes\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy.typing import ArrayLike\n",
    "import pandas as pd\n",
    "from sklearn.metrics import (\n",
    "  classification_report, ConfusionMatrixDisplay, PrecisionRecallDisplay\n",
    ")\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.pipeline import Pipeline\n",
    "import sklearn.tree as sk_tree\n",
    "import torch\n",
    "from transformers.models.bert import (\n",
    "  BertForSequenceClassification, BertTokenizer\n",
    ")\n",
    "\n",
    "from common.config import misc\n",
    "from common.types import FeatureImportances, HamSpamFeatureImportances\n",
    "from pipeline.text_classifier_builder import TextClassifierBuilder\n",
    "from pipeline.utils import get_predictor, get_predictor_name, get_transformers\n",
    "from tasks.ada_boost_task import (\n",
    "  AdaBoostClassifierBuilder, AdaBoostTask\n",
    ")\n",
    "import tasks.bert_task as bert_task\n",
    "from tasks.best_bow_task import BestBowTask\n",
    "from tasks.decision_tree_task import (\n",
    "  DecisionTreeClassifierBuilder, DecisionTreeTask\n",
    ")\n",
    "from tasks.email_preprocess_task import EmailPreprocessTask\n",
    "from tasks.extra_trees_task import (\n",
    "  ExtraTreesClassifierBuilder, ExtraTreesTask\n",
    ")\n",
    "from tasks.gradient_boosting_task import (\n",
    "  GradientBoostingClassifierBuilder, GradientBoostingTask\n",
    ")\n",
    "from tasks.linear_svm_task import (\n",
    "  LinearSvmClassifierBuilder, LinearSvmTask\n",
    ")\n",
    "from tasks.logistic_regression_task import (\n",
    "  LogisticRegressionClassifierBuilder, LogisticRegressionTask\n",
    ")\n",
    "from tasks.naive_bayes_task import (\n",
    "  NaiveBayesClassifierBuilder, NaiveBayesTask\n",
    ")\n",
    "from tasks.nltk_task import NltkTask\n",
    "from tasks.poly_svm_task import (\n",
    "  PolySvmClassifierBuilder, PolySvmTask\n",
    ")\n",
    "from tasks.random_forest_task import (\n",
    "  RandomForestClassifierBuilder, RandomForestTask\n",
    ")\n",
    "from tasks.rbf_svm_task import (\n",
    "  RbfSvmClassifierBuilder, RbfSvmTask\n",
    ")\n",
    "from tasks.sms_preprocess_task import SmsPreprocessTask\n",
    "from tasks.stacking_task import (\n",
    "  StackingClassifierBuilder, StackingTask\n",
    ")\n",
    "from tasks.voting_task import (\n",
    "  VotingClassifierBuilder, VotingTask\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZRMCpGDYAP39"
   },
   "source": [
    "# Demonstration of methods for message spam detection\n",
    "## Introduction\n",
    "This project demonstrates and compares various message spam detection methods by implementing a general ham/spam binary classification task on datasets obtained from:\n",
    "- [SpamAssassin public mail corpus](#SpamAssassin-public-mail-corpus)\n",
    "- [UCI Machine Learning Repository](#SMS-Spam-Collection---UCI-Machine-Learning-Repository)\n",
    "\n",
    "The ham/spam binary classification task is implemented by utilization of the following two groups of [machine learning models](#Machine-learning-models):\n",
    "- [bag-of-words (BoW)](#Bag-of-words)\n",
    "  - [naive Bayes](#Naive-Bayes)\n",
    "  - [logistic regression](#Logistic-regression)\n",
    "  - [decision trees](#Decision-tree)\n",
    "  - [support vector machines](#Support-vector-machine)\n",
    "  - [ensembles - bagging (voting), boosting, stacking](#Ensemble-learning)\n",
    "- [bidirectional encoder representations from transformers (BERT)](#BERT)\n",
    "\n",
    "The message spam data is modeled in two different ways depending on the classification approach:\n",
    "- [term frequencyâ€“inverse document frequency (`TF-IDF`)](#TF-IDF), a \"normalized\" `BoW` text model,\n",
    "  together with custom [feature engineering](#Feature-engineering)\n",
    "- [word embeddings for neural natural language processing](#Word-embedding)\n",
    "\n",
    "Because of the wide-spread application of the `BoW` approach for text classification and the presence of many `BoW`-based methods (some of them mentioned above), the main focus is to build a high-performance `BoW` classifier, while a `BERT` classifier is used mainly to point out the specifics of using `BERT` as an alternative approach for text classification. So the `BoW` model with best metrics is selected for comparison with a `BERT` model, though some [limitations](#Limitations) apply.\n",
    "\n",
    "The implementation of the `BoW` classifiers is based on [Scikit-learn](#Scikit-learn) and the `Scikit-learn` terms for *estimator*, *transformer* and *predictor* are used.\n",
    "The implementation of the `BERT` classifier is based on [Hugging Face Transformers](#Hugging-Face-Transformers) and [PyTorch](#PyTorch).\n",
    "\n",
    "There are multiple trade-offs between the `BoW` and `BERT` approaches, and this project attempts to empirically show the strong and weak points of each of the two approaches.\n",
    "\n",
    "`BoW` models can work with very long text sequences at the expense of reduced contextual interpretation of word order and grouping. For the latter, as a compensation, `BoW` models can statistically process groups of word-tokens (`n-grams`) but this is not a substitute for technologies like `word2vec`, [Recurrent Neural Networks (RNNs)](#Recurrent-neural-network) and [Attention](#Attention), which are applied in neural networks like `BERT` and together allow for capturing phrases or bidirectional interdependencies within a sequence of input word-tokens.\n",
    "\n",
    "Additionally, `BoW` models provide better explainability on how their algorithm works and how the classification output is created.\n",
    "\n",
    "<br><br>\n",
    "\n",
    "## File structure\n",
    "### Folders (packages)\n",
    "- *common*: General configuration and types.\n",
    "- *models*: Saved `BoW` and `BERT` models.\n",
    "- *pipeline*: Training a `BoW` text classifier based on a `Scikit-learn` pipeline.\n",
    "- *tasks*: [Luigi](#Luigi) tasks and builders of classifiers leveraging [Optuna](#Optuna) and related utilities.\n",
    "### This notebook\n",
    "- Analysis of text classifiers already built by [Luigi](#Luigi) tasks.\n",
    "### Other files\n",
    "- Various configuration and setup files, such as *luigi.cfg*.\n",
    "\n",
    "<a name=\"Limitations\"></a>\n",
    "## Limitations\n",
    "If in *luigi.cfg* is set `classification.feature_selector_type=svd` and a `BoW` model is built to use a feature selector, then `BoW`-`BERT` comparison is not supported for this `BoW` model. This limitation is due to unavailable `TF-IDF` feature information when dimensionality reduction is applied before the classifier in a pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbuWm8EnJCG8"
   },
   "source": [
    "## Data retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMVUmyOcAQYx"
   },
   "source": [
    "### Data distribution utilities\n",
    "\n",
    "Here are some basic utilities that can give an overview on how the message data is distributed: in terms of letter, language, size, duplicates and ham/spam label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ta-EFu3xAQYy"
   },
   "outputs": [],
   "source": [
    "def plot_letter_distribution(\n",
    "  df: pd.DataFrame,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  counter = Counter()  # type: ignore\n",
    "  for m in df.message:\n",
    "    counter.update(c.lower() for c in m if c.isalpha())\n",
    "  letters, counts = zip(*counter.most_common(50))\n",
    "  axes.bar(letters, counts)\n",
    "  axes.set_xlabel(\"Letter\")\n",
    "  axes.set_ylabel(\"Count\")\n",
    "  axes.set_title(title)\n",
    "\n",
    "\n",
    "def plot_language_distribution(\n",
    "  df: pd.DataFrame,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  all_count = len(df)\n",
    "  all_english_words = NltkTask().all_english_words\n",
    "  english_count = df.message.apply(\n",
    "    lambda m: is_likely_english_text(m, all_english_words)\n",
    "  ).sum()\n",
    "  nonenglish_count = all_count - english_count\n",
    "  language_distribution = [english_count, nonenglish_count]\n",
    "  axes.pie(\n",
    "    language_distribution,\n",
    "    labels=[f\"English ({english_count})\",\n",
    "            f\"other ({nonenglish_count})\"],\n",
    "    autopct=\"%.2f%%\",\n",
    "  )\n",
    "  axes.set_title(title)\n",
    "\n",
    "\n",
    "def is_likely_english_text(\n",
    "  text: str,\n",
    "  all_english_words: set[str]\n",
    ") -> bool:\n",
    "  if not text:\n",
    "    return False\n",
    "\n",
    "  words_count = 0\n",
    "  english_words_count = 0\n",
    "  for token in text.lower().split():\n",
    "    if token.isalpha():\n",
    "      if token in all_english_words:\n",
    "        english_words_count += 1\n",
    "      words_count += 1\n",
    "  return (\n",
    "    words_count != 0\n",
    "    and 0.67 < (english_words_count / words_count)\n",
    "  )\n",
    "\n",
    "\n",
    "def plot_size_distribution(\n",
    "  df: pd.DataFrame,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  message_sizes = df.message.str.split().apply(lambda x: len(x))\n",
    "  ham_sizes = message_sizes[df.is_spam == 0]\n",
    "  spam_sizes = message_sizes[df.index.difference(ham_sizes.index)]\n",
    "  axes.hist(ham_sizes, bins=\"auto\", label=\"Ham\", alpha=0.5)\n",
    "  axes.hist(spam_sizes, bins=\"auto\", label=\"Spam\", alpha=0.5)\n",
    "  axes.semilogy()\n",
    "  axes.set_title(title)\n",
    "  axes.legend()\n",
    "\n",
    "\n",
    "def plot_duplicates_distribution(\n",
    "  df: pd.DataFrame,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  all_count = len(df)\n",
    "  duplicated = df[df.duplicated]\n",
    "  spam_duplicates_count = duplicated.is_spam.sum()\n",
    "  ham_duplicates_count = len(duplicated) - spam_duplicates_count\n",
    "  uniques_count = (\n",
    "    all_count - ham_duplicates_count - spam_duplicates_count\n",
    "  )\n",
    "  duplicates_distribution = [\n",
    "    ham_duplicates_count, spam_duplicates_count, uniques_count\n",
    "  ]\n",
    "  axes.pie(\n",
    "    duplicates_distribution,\n",
    "    explode=[0, 0.5, 0],\n",
    "    labels=[f\"Ham Duplicates ({ham_duplicates_count})\",\n",
    "            f\"Spam Duplicates ({spam_duplicates_count})\",\n",
    "            f\"Uniques ({uniques_count})\"],\n",
    "    autopct=\"%.2f%%\",\n",
    "  )\n",
    "  axes.set_title(title)\n",
    "\n",
    "\n",
    "def plot_spam_distribution(\n",
    "  df: pd.DataFrame,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  all_count = len(df)\n",
    "  spam_count = df.is_spam.sum()\n",
    "  ham_count = all_count - spam_count\n",
    "  ham_spam_distribution = [ham_count, spam_count]\n",
    "  axes.pie(\n",
    "    ham_spam_distribution,\n",
    "    labels=[f\"Ham ({ham_count})\", f\"Spam ({spam_count})\"],\n",
    "    autopct=\"%.2f%%\",\n",
    "  )\n",
    "  axes.set_title(title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-dmoy8CqzRvf"
   },
   "source": [
    "### Email dataset\n",
    "\n",
    "In this section are displayed letter, language, size, duplicates and ham/spam distribution characteristics on the output of the `tasks.email_preprocess_task.EmailPreprocessTask` task.\n",
    "\n",
    "The email letter distribution shows that the most frequent letters are part of the English alphabet, and non-English letters are much less frequent. The latter observation is confirmed by the email language statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "email_spam_data_path = Path(EmailPreprocessTask().output().path)\n",
    "if Path.exists(email_spam_data_path):\n",
    "  email_spam_df = pd.read_csv(email_spam_data_path)\n",
    "  _, (ax_size, ax_dup,\n",
    "      ax_spam, ax_letter, ax_lang) = plt.subplots(5, 1, figsize=(5, 25))\n",
    "  plot_size_distribution(\n",
    "    email_spam_df,\n",
    "    \"Email sizes distribution\",\n",
    "    ax_size,\n",
    "  )\n",
    "  plot_duplicates_distribution(\n",
    "    email_spam_df,\n",
    "    \"Email spam duplicates distribution\",\n",
    "    ax_dup,\n",
    "  )\n",
    "  plot_spam_distribution(\n",
    "    email_spam_df,\n",
    "    \"Email spam distribution\",\n",
    "    ax_spam,\n",
    "  )\n",
    "  plot_letter_distribution(\n",
    "    email_spam_df,\n",
    "    \"Email letter distribution\",\n",
    "    ax_letter,\n",
    "  )\n",
    "  plot_language_distribution(\n",
    "    email_spam_df,\n",
    "    \"Email language distribution\",\n",
    "    ax_lang,\n",
    "  )\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SMS dataset\n",
    "\n",
    "In this section are displayed letter, language, size, duplicates and ham/spam distribution characteristics on the output of the `tasks.sms_preprocess_task.SmsPreprocessTask` task.\n",
    "\n",
    "The SMS letter distribution shows that the most frequent letters are part of the English alphabet, and non-English letters are much less frequent. The latter observation is confirmed by the SMS language statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sms_spam_data_path = Path(SmsPreprocessTask().output().path)\n",
    "if Path.exists(sms_spam_data_path):\n",
    "  sms_spam_df = pd.read_csv(sms_spam_data_path)\n",
    "  _, (ax_size, ax_dup,\n",
    "      ax_spam, ax_letter, ax_lang) = plt.subplots(5, 1, figsize=(5, 25))\n",
    "  plot_size_distribution(\n",
    "    sms_spam_df,\n",
    "    \"SMS sizes distribution\",\n",
    "    ax_size,\n",
    "  )\n",
    "  plot_duplicates_distribution(\n",
    "    sms_spam_df,\n",
    "    \"SMS spam duplicates distribution\",\n",
    "    ax_dup,\n",
    "  )\n",
    "  plot_spam_distribution(\n",
    "    sms_spam_df,\n",
    "    \"SMS spam distribution\",\n",
    "    ax_spam,\n",
    "  )\n",
    "  plot_letter_distribution(\n",
    "    sms_spam_df,\n",
    "    \"SMS letter distribution\",\n",
    "    ax_letter,\n",
    "  )\n",
    "  plot_language_distribution(\n",
    "    sms_spam_df,\n",
    "    \"SMS language distribution\",\n",
    "    ax_lang,\n",
    "  )\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Message dataset\n",
    "\n",
    "In this section is loaded the output of the `tasks.train_test_split.TrainTestSplit` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QodNU6BRJhtD"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/train_messages.csv\")\n",
    "X_train, y_train = train_df.message, train_df.is_spam\n",
    "\n",
    "test_df = pd.read_csv(\"data/test_messages.csv\")\n",
    "X_test, y_test = test_df.message, test_df.is_spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common evaluatory functions\n",
    "\n",
    "In this section are common metrics visualization functions:\n",
    "  - `plot_confusion_matrix`: utilizes `sklearn.metrics.ConfusionMatrixDisplay.from_predictions`;\n",
    "  - `plot_precision_recall_curve`: utilizes `sklearn.metrics.PrecisionRecallDisplay.from_predictions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(\n",
    "  y: ArrayLike,\n",
    "  predictions: ArrayLike,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  ConfusionMatrixDisplay.from_predictions(\n",
    "    y, predictions,\n",
    "    labels=[0, 1], normalize=\"true\",\n",
    "    display_labels=[\"Ham\", \"Spam\"],\n",
    "    values_format=\".0%\", ax=axes,\n",
    "  )\n",
    "  axes.set_title(title)\n",
    "\n",
    "\n",
    "def plot_precision_recall_curve(\n",
    "  y: ArrayLike,\n",
    "  predictions: ArrayLike,\n",
    "  title: str,\n",
    "  axes: Axes,\n",
    ") -> None:\n",
    "  PrecisionRecallDisplay.from_predictions(\n",
    "    y, predictions,\n",
    "    pos_label=1, name=title, ax=axes,\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "leeztUFpqgkC"
   },
   "source": [
    "## BoW approach\n",
    "\n",
    "This section, together with its subsections, contains research that is specific to the `BoW` approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wAnrZxqfAimD"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "In this section are defined some evaluatory functions intended for `BoW` classifiers, such as:\n",
    "- `show_scores`: displays metrics such as classification report, confusion matrix and precision-recall curve;\n",
    "- `show_top_features`: displays and optionally returns separately `TOP_K_FEATURES` ham and `TOP_K_FEATURES` spam features, `TOP_K_FEATURES` ham and/or spam features, or *None*;\n",
    "- `get_top_log_proba_features_by_label`, `get_top_reduced_features`, `get_top_reduced_features_by_label`, `get_top_weighted_features`, `get_top_weighted_features_by_label`: all used by `show_top_features`.\n",
    "- `performs_feature_reduction`: tests whether a `BoW` classifier incorporates dimensionality reduction, used by `show_top_features`;\n",
    "- `show_feature_importances`: a utility function that displays the feature importances, used by `show_top_features`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Te-9oFHgqgkG"
   },
   "outputs": [],
   "source": [
    "TOP_K_FEATURES = 10\n",
    "\n",
    "\n",
    "def show_scores(\n",
    "  model: Pipeline,\n",
    "  X: pd.Series = X_test,\n",
    "  y: pd.Series = y_test,\n",
    ") -> None:\n",
    "  \"\"\"Displays various metrics for `model`.\n",
    "  \n",
    "  Displayed metrics are:\n",
    "  - classification report;\n",
    "  - confusion matrix;\n",
    "  - precision-recall curve.\n",
    "\n",
    "  Predictions used for precision-recall curve are obtained from\n",
    "  `predict_proba`, `decision_function` or `predict` - whichever\n",
    "  method is available, in order of priority.\n",
    "\n",
    "  Args:\n",
    "    model: A `Scikit-learn` pipeline.\n",
    "    X: The messages.\n",
    "    y: The ham/spam labels.\n",
    "  \"\"\"\n",
    "  predictions = model.predict(X)\n",
    "  predictor_name = get_predictor_name(model)\n",
    "  classification_report_str = classification_report(\n",
    "    y, predictions,\n",
    "    labels=[0, 1], target_names=[\"Ham\", \"Spam\"],\n",
    "    digits=3, zero_division=np.nan,\n",
    "  )\n",
    "  print(f\"\\n\\n{predictor_name}:\")\n",
    "  print(\"-\" * 80)\n",
    "  print(classification_report_str)\n",
    "  if hasattr(model, \"predict_proba\"):\n",
    "    prob_predictions = model.predict_proba(X)[:, 1]\n",
    "  elif hasattr(model, \"decision_function\"):\n",
    "    prob_predictions = model.decision_function(X)\n",
    "  else:\n",
    "    prob_predictions = predictions\n",
    "  _, (ax_cm, ax_prc) = plt.subplots(2, 1, figsize=(8, 12))\n",
    "  plot_confusion_matrix(y, predictions,\n",
    "                        predictor_name, ax_cm)\n",
    "  plot_precision_recall_curve(y, prob_predictions,\n",
    "                              predictor_name, ax_prc)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "def show_top_features(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    "  return_features: bool = False,\n",
    ") -> HamSpamFeatureImportances | FeatureImportances | None:\n",
    "  \"\"\"Displays and optionally returns top features and their importances.\n",
    "\n",
    "  What features are displayed (returned) depends on attributes of `model`.\n",
    "  In the case of dimensionality reduction, the top features are derived from\n",
    "  a single projected feature which is prioritized based on both variance and\n",
    "  feature importance metrics.\n",
    "\n",
    "  Args:\n",
    "    model: A `Scikit-learn` pipeline.\n",
    "    k: Number of selected top features.\n",
    "    return_features: Whether to return the features.\n",
    "\n",
    "  Returns:\n",
    "    Separately `TOP_K_FEATURES` ham and `TOP_K_FEATURES` spam features,\n",
    "    `TOP_K_FEATURES` ham and/or spam features, or *None*.\n",
    "\n",
    "  Raises:\n",
    "    ValueError: If no suitable metric is found.\n",
    "  \"\"\"\n",
    "  predictor = get_predictor(model)\n",
    "  has_feature_log_prob_ = hasattr(predictor, \"feature_log_prob_\")\n",
    "  has_coef_ = hasattr(predictor, \"coef_\")\n",
    "  has_ham_spam_importances = has_feature_log_prob_ or has_coef_\n",
    "  if has_ham_spam_importances:\n",
    "    if has_feature_log_prob_:\n",
    "      top_k_features_by_label = get_top_log_proba_features_by_label(model, k)\n",
    "    else:\n",
    "      if performs_feature_reduction(model):\n",
    "        top_k_features_by_label = get_top_reduced_features_by_label(model, k)\n",
    "      else:\n",
    "        top_k_features_by_label = get_top_weighted_features_by_label(model, k)\n",
    "    show_feature_importances(top_k_features_by_label[0],\n",
    "                             \"Most important ham features:\")\n",
    "    show_feature_importances(top_k_features_by_label[1],\n",
    "                             \"Most important spam features:\")\n",
    "    return top_k_features_by_label if return_features else None\n",
    "  elif hasattr(predictor, \"feature_importances_\"):\n",
    "    if performs_feature_reduction(model):\n",
    "      top_k_features = get_top_reduced_features(model, k)\n",
    "    else:\n",
    "      top_k_features = get_top_weighted_features(model, k)\n",
    "    show_feature_importances(top_k_features, \"Most important features:\")\n",
    "    return top_k_features if return_features else None\n",
    "  else:\n",
    "    raise ValueError(\"No metric available for getting the top features\")\n",
    "\n",
    "\n",
    "def get_top_log_proba_features_by_label(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    ") -> HamSpamFeatureImportances:\n",
    "  transformers = get_transformers(model)\n",
    "  names = transformers.get_feature_names_out()\n",
    "  predictor = get_predictor(model)\n",
    "  importances = predictor.feature_log_prob_\n",
    "  ham_top_k_indices = np.argsort(importances[0])[-k:][::-1]\n",
    "  spam_top_k_indices = np.argsort(importances[1])[-k:][::-1]\n",
    "  ham_top_k_names = names[ham_top_k_indices]\n",
    "  spam_top_k_names = names[spam_top_k_indices]\n",
    "  ham_top_k_importances = importances[0][ham_top_k_indices]\n",
    "  spam_top_k_importances = importances[1][spam_top_k_indices]\n",
    "  return (list(zip(ham_top_k_names, ham_top_k_importances)),\n",
    "          list(zip(spam_top_k_names, spam_top_k_importances)))\n",
    "\n",
    "\n",
    "def get_top_reduced_features(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    ") -> FeatureImportances:\n",
    "  names = model.named_steps[\"Features\"].get_feature_names_out()\n",
    "  svd = model.named_steps[\"Feature Selector\"][\"truncatedsvd\"]\n",
    "  predictor = get_predictor(model)\n",
    "  importances = predictor.feature_importances_\n",
    "  svd_top_k_indices = np.argsort(importances)[-k:]\n",
    "  best_svd_component = svd.components_[np.min(svd_top_k_indices)]\n",
    "  top_k_indices = np.argsort(best_svd_component)[-k:][::-1]\n",
    "  top_k_names = names[top_k_indices]\n",
    "  top_k_importances = best_svd_component[top_k_indices]\n",
    "  return list(zip(top_k_names, top_k_importances))\n",
    "\n",
    "\n",
    "def get_top_reduced_features_by_label(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    ") -> HamSpamFeatureImportances:\n",
    "  names = model.named_steps[\"Features\"].get_feature_names_out()\n",
    "  svd = model.named_steps[\"Feature Selector\"][\"truncatedsvd\"]\n",
    "  predictor = get_predictor(model)\n",
    "  svd_importances = predictor.coef_.ravel()\n",
    "  svd_top_indices = np.argsort(svd_importances)\n",
    "  svd_ham_top_k_indices = svd_top_indices[:k]\n",
    "  svd_spam_top_k_indices = svd_top_indices[-k:]\n",
    "  svd_best_ham_component = svd.components_[\n",
    "    np.min(svd_ham_top_k_indices)\n",
    "  ]\n",
    "  svd_best_spam_component = svd.components_[\n",
    "    np.min(svd_spam_top_k_indices)\n",
    "  ]\n",
    "  ham_top_k_indices = np.argsort(svd_best_ham_component)[-k:][::-1]\n",
    "  spam_top_k_indices = np.argsort(svd_best_spam_component)[-k:][::-1]\n",
    "  ham_top_k_names = names[ham_top_k_indices]\n",
    "  spam_top_k_names = names[spam_top_k_indices]\n",
    "  ham_top_k_importances = svd_best_ham_component[ham_top_k_indices]\n",
    "  spam_top_k_importances = svd_best_spam_component[spam_top_k_indices]\n",
    "  return (list(zip(ham_top_k_names, ham_top_k_importances)),\n",
    "          list(zip(spam_top_k_names, spam_top_k_importances)))\n",
    "\n",
    "\n",
    "def get_top_weighted_features(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    ") -> FeatureImportances:\n",
    "  transformers = get_transformers(model)\n",
    "  names = transformers.get_feature_names_out()\n",
    "  predictor = get_predictor(model)\n",
    "  importances = predictor.feature_importances_\n",
    "  top_k_indices = np.argsort(importances)[-k:][::-1]\n",
    "  top_k_names = names[top_k_indices]\n",
    "  top_k_importances = importances[top_k_indices]\n",
    "  return list(zip(top_k_names, top_k_importances))\n",
    "\n",
    "\n",
    "def get_top_weighted_features_by_label(\n",
    "  model: Pipeline,\n",
    "  k: int = TOP_K_FEATURES,\n",
    ") -> HamSpamFeatureImportances:\n",
    "  transformers = get_transformers(model)\n",
    "  names = transformers.get_feature_names_out()\n",
    "  predictor = get_predictor(model)\n",
    "  importances = predictor.coef_.ravel()\n",
    "  top_indices = np.argsort(importances)\n",
    "  ham_top_k_indices = top_indices[:k]\n",
    "  spam_top_k_indices = top_indices[-k:][::-1]\n",
    "  ham_top_k_names = names[ham_top_k_indices]\n",
    "  spam_top_k_names = names[spam_top_k_indices]\n",
    "  ham_top_k_importances = importances[ham_top_k_indices]\n",
    "  spam_top_k_importances = importances[spam_top_k_indices]\n",
    "  return (list(zip(ham_top_k_names, ham_top_k_importances)),\n",
    "          list(zip(spam_top_k_names, spam_top_k_importances)))\n",
    "\n",
    "\n",
    "def performs_feature_reduction(model: Pipeline) -> bool:\n",
    "  if \"Feature Selector\" in model.named_steps:\n",
    "    feature_selector = model.named_steps[\"Feature Selector\"]\n",
    "    return (hasattr(feature_selector, \"named_steps\")\n",
    "            and \"truncatedsvd\" in feature_selector.named_steps)\n",
    "  else:\n",
    "    return False\n",
    "\n",
    "\n",
    "def show_feature_importances(\n",
    "  top_features: FeatureImportances,\n",
    "  title: str,\n",
    ") -> None:\n",
    "  print(f\"\\n\\n{title}\")\n",
    "  print(\"-\" * 80)\n",
    "  for name, importance in top_features:\n",
    "    print(f\"{name} -> {importance:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CvyldYtHqgkG"
   },
   "source": [
    "#### Naive Bayes classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `naive_bayes_classifier` which is the output of a `tasks.naive_bayes_task.NaiveBayesTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "95g6QeKRqgkH"
   },
   "outputs": [],
   "source": [
    "naive_bayes_buider = NaiveBayesClassifierBuilder()\n",
    "naive_bayes_classifier = naive_bayes_buider.build(\n",
    "  NaiveBayesTask().output().path\n",
    ")\n",
    "show_top_features(naive_bayes_classifier)\n",
    "show_scores(naive_bayes_classifier)\n",
    "naive_bayes_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xscUsVFggdvG"
   },
   "source": [
    "#### Logistic regression classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `logistic_regression_classifier` which is the output of a `tasks.logistic_regression_task.LogisticRegressionTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cEbWjOv0qgkL"
   },
   "outputs": [],
   "source": [
    "logistic_regression_builder = LogisticRegressionClassifierBuilder()\n",
    "logistic_regression_classifier = logistic_regression_builder.build(\n",
    "  LogisticRegressionTask().output().path\n",
    ")\n",
    "show_top_features(logistic_regression_classifier)\n",
    "show_scores(logistic_regression_classifier)\n",
    "logistic_regression_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PNQafaz4xdJB"
   },
   "source": [
    "#### Decision tree classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `decision_tree_classifier` which is the output of a `tasks.decision_tree_task.DecisionTreeTask`.\n",
    "\n",
    "The decision tree structure is visualized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUIkMqMEmp97"
   },
   "outputs": [],
   "source": [
    "def visualize_tree(model: Pipeline) -> None:\n",
    "  transformers = get_transformers(model)\n",
    "  predictor = get_predictor(model)\n",
    "  print(\n",
    "    sk_tree.export_text(\n",
    "      predictor,\n",
    "      feature_names=transformers.get_feature_names_out(),\n",
    "      class_names=[\"Ham\", \"Spam\"],\n",
    "      max_depth=predictor.tree_.max_depth,\n",
    "    )\n",
    "  )\n",
    "\n",
    "\n",
    "decision_tree_builder = DecisionTreeClassifierBuilder()\n",
    "decision_tree_classifier = decision_tree_builder.build(\n",
    "  DecisionTreeTask().output().path\n",
    ")\n",
    "show_top_features(decision_tree_classifier)\n",
    "show_scores(decision_tree_classifier)\n",
    "visualize_tree(decision_tree_classifier)\n",
    "decision_tree_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f21HaCCTxkWE"
   },
   "source": [
    "#### SVM classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7KIUokeTqgkM"
   },
   "source": [
    "##### Linear SVM classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `linear_svm_classifier` which is the output of a `tasks.linear_svm_task.LinearSvmTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w9KugoFHqgkM"
   },
   "outputs": [],
   "source": [
    "linear_svm_builder = LinearSvmClassifierBuilder()\n",
    "linear_svm_classifier = linear_svm_builder.build(\n",
    "  LinearSvmTask().output().path\n",
    ")\n",
    "linear_svm_top_features = show_top_features(linear_svm_classifier,\n",
    "                                            return_features=True)\n",
    "show_scores(linear_svm_classifier)\n",
    "linear_svm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQaRwbOYqgkM"
   },
   "source": [
    "##### RBF SVM classifier\n",
    "\n",
    "Here are displayed various metrics for `rbf_svm_classifier` which is the output of a `tasks.rbf_svm_task.RbfSvmTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FdReLpcrmp98"
   },
   "outputs": [],
   "source": [
    "rbf_svm_builder = RbfSvmClassifierBuilder()\n",
    "rbf_svm_classifier = rbf_svm_builder.build(\n",
    "  RbfSvmTask().output().path\n",
    ")\n",
    "show_scores(rbf_svm_classifier)\n",
    "rbf_svm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jMipUxuvqgkM"
   },
   "source": [
    "##### Polynomial SVM classifier\n",
    "\n",
    "Here are displayed various metrics for `poly_svm_classifier` which is the output of a `tasks.poly_svm_task.PolySvmTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SxoHilcsqgkN"
   },
   "outputs": [],
   "source": [
    "poly_svm_builder = PolySvmClassifierBuilder()\n",
    "poly_svm_classifier = poly_svm_builder.build(\n",
    "  PolySvmTask().output().path\n",
    ")\n",
    "show_scores(poly_svm_classifier)\n",
    "poly_svm_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jGoAX0cQqgkN"
   },
   "source": [
    "#### Bagging classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2WCkZ3ryqgkN"
   },
   "source": [
    "##### Random forest classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `random_forest_classifier` which is the output of a `tasks.random_forest_task.RandomForestTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_3GuEJ7qgkN"
   },
   "outputs": [],
   "source": [
    "random_forest_builder = RandomForestClassifierBuilder()\n",
    "random_forest_classifier = random_forest_builder.build(\n",
    "  RandomForestTask().output().path\n",
    ")\n",
    "show_top_features(random_forest_classifier)\n",
    "show_scores(random_forest_classifier)\n",
    "random_forest_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5-qpN-jqgkN"
   },
   "source": [
    "##### Extra-trees classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `extra_trees_classifier` which is the output of a `tasks.extra_trees_task.ExtraTreesTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AXsmQSoNqgkN"
   },
   "outputs": [],
   "source": [
    "extra_trees_builder = ExtraTreesClassifierBuilder()\n",
    "extra_trees_classifier = extra_trees_builder.build(\n",
    "  ExtraTreesTask().output().path\n",
    ")\n",
    "show_top_features(extra_trees_classifier)\n",
    "show_scores(extra_trees_classifier)\n",
    "extra_trees_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ricm0rSCUqyw"
   },
   "source": [
    "#### Boosting classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LoIN3OsqgkO"
   },
   "source": [
    "##### Adaptive boosting classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `ada_boost_classifier` which is the output of a `tasks.ada_boost_task.AdaBoostTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lYlUkpObqgkO"
   },
   "outputs": [],
   "source": [
    "ada_boost_builder = AdaBoostClassifierBuilder()\n",
    "ada_boost_classifier = ada_boost_builder.build(\n",
    "  AdaBoostTask().output().path\n",
    ")\n",
    "show_top_features(ada_boost_classifier)\n",
    "show_scores(ada_boost_classifier)\n",
    "ada_boost_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rG7E1Bd-qgkO"
   },
   "source": [
    "##### Gradient boosting classifier\n",
    "\n",
    "Here are displayed top features and various metrics for `gradient_boosting_classifier` which is the output of a `tasks.gradient_boosting_task.GradientBoostingTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_g32SpVSUtWN"
   },
   "outputs": [],
   "source": [
    "gradient_boosting_builder = GradientBoostingClassifierBuilder()\n",
    "gradient_boosting_classifier = gradient_boosting_builder.build(\n",
    "  GradientBoostingTask().output().path\n",
    ")\n",
    "show_top_features(gradient_boosting_classifier)\n",
    "show_scores(gradient_boosting_classifier)\n",
    "gradient_boosting_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AFH-CzKpAq3e"
   },
   "source": [
    "#### Voting classifier\n",
    "\n",
    "Here are displayed various metrics for `voting_classifier` which is the output of a `tasks.voting_task.VotingTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u9cwk4x5An5Y"
   },
   "outputs": [],
   "source": [
    "voting_builder = VotingClassifierBuilder()\n",
    "voting_classifier = voting_builder.build(\n",
    "  VotingTask().output().path\n",
    ")\n",
    "show_scores(voting_classifier)\n",
    "voting_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NiuJ-TwdtJS8"
   },
   "source": [
    "#### Stacking classifier\n",
    "\n",
    "Here are displayed various metrics for `stacking_classifier` which is the output of a `tasks.stacking_task.StackingTask`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dBkCvvedtOo3"
   },
   "outputs": [],
   "source": [
    "stacking_builder = StackingClassifierBuilder()\n",
    "stacking_classifier = stacking_builder.build(\n",
    "  StackingTask().output().path\n",
    ")\n",
    "show_scores(stacking_classifier)\n",
    "stacking_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOQ-QyNGYm_L"
   },
   "source": [
    "### Best BoW classifier\n",
    "\n",
    "Here are displayed various metrics for `best_bow_classifier` which is the output of a `tasks.best_bow_task.BestBowTask`.\n",
    "\n",
    "Also displayed is another `tasks.best_bow_task.BestBowTask` output: the scores of all `BoW` classifiers that have competed with `best_bow_classifier`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lTC66iWKY4eZ"
   },
   "outputs": [],
   "source": [
    "bow_classifier_scores = plt.imread(\n",
    "  BestBowTask().output()[\"bow_classifier_scores\"].path\n",
    ")\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(bow_classifier_scores)\n",
    "plt.show()\n",
    "\n",
    "best_bow_builder = TextClassifierBuilder()\n",
    "best_bow_classifier = best_bow_builder.build(\n",
    "  BestBowTask().output()[\"best_bow_classifier\"].path\n",
    ")\n",
    "show_scores(best_bow_classifier)\n",
    "best_bow_classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yuzB05FU3Q4Y"
   },
   "source": [
    "## BERT approach\n",
    "\n",
    "This section, together with its subsections, contains research that is specific to the `BERT` approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocessing\n",
    "\n",
    "In order for the best `BoW` model to be compared with a `BERT` model, via `bert_task.get_bow_vocabulary` and `bert_task.get_bow_dataset` `X_test` is converted to `bert_test_df`, a `pandas.DataFrame` based on `best_bow_classifier`'s vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if performs_feature_reduction(best_bow_classifier):\n",
    "  raise NotImplementedError(\n",
    "    \"No TF-IDF features after dimensionality reduction.\"\n",
    "  )\n",
    "\n",
    "bow_vocabulary = bert_task.get_bow_vocabulary(best_bow_classifier)\n",
    "bert_test_df = bert_task.get_bow_dataset(\n",
    "  best_bow_classifier, bow_vocabulary,\n",
    "  X_test, y_test,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dIch3wYPqgkP"
   },
   "source": [
    "### Evaluation\n",
    "\n",
    "In order to support comparisons between `BoW` and `BERT` classifiers, in this section:\n",
    "- the already trained `bert_classifier` is loaded via `transformers.BertForSequenceClassification.from_pretrained`;\n",
    "- to generate predictions from `bert_classifier`, `bert_test_df` is converted to `datasets.Dataset` format and further processed by `bert_tokenizer`.\n",
    "\n",
    "Based on `get_bert_predictions` are displayed various metrics for `bert_classifier`: classification report, confusion matrix and precision-recall curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9N9U5DIiVuM"
   },
   "outputs": [],
   "source": [
    "def get_bert_predictions(\n",
    "  bert_classifier: BertForSequenceClassification,\n",
    "  bert_tokenizer: BertTokenizer,\n",
    "  X: datasets.Dataset,\n",
    ") -> tuple[torch.Tensor, torch.Tensor]:\n",
    "  inputs = bert_tokenizer(list(X[\"message\"]),\n",
    "                          padding=\"max_length\",\n",
    "                          truncation=True,\n",
    "                          return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    bert_prob_predictions = torch.softmax(\n",
    "      bert_classifier(**inputs).logits, dim=1\n",
    "    )[:, 1]\n",
    "    bert_predictions = (bert_prob_predictions >= 0.5).type(torch.int32)\n",
    "  return bert_prob_predictions, bert_predictions\n",
    "\n",
    "bert_model_path = bert_task.BertTask().output()[\"bert_classifier_model\"].path\n",
    "bert_classifier = BertForSequenceClassification.from_pretrained(\n",
    "  bert_model_path\n",
    ")\n",
    "bert_tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "X_bert_test = datasets.Dataset.from_pandas(bert_test_df)\n",
    "bert_prob_predictions, bert_predictions = get_bert_predictions(\n",
    "  bert_classifier, bert_tokenizer, X_bert_test\n",
    ")\n",
    "classification_report_str = classification_report(\n",
    "  y_test, bert_predictions,\n",
    "  labels=[0, 1], target_names=[\"Ham\", \"Spam\"],\n",
    "  digits=3, zero_division=np.nan,\n",
    ")\n",
    "print(f\"\\n\\nBERT:\")\n",
    "print(\"-\" * 80)\n",
    "print(classification_report_str)\n",
    "_, (ax_cm, ax_prc) = plt.subplots(2, 1, figsize=(8, 12))\n",
    "plot_confusion_matrix(y_test, bert_predictions,\n",
    "                      \"BERT\", ax_cm)\n",
    "plot_precision_recall_curve(y_test, bert_prob_predictions,\n",
    "                            \"BERT\", ax_prc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0rM1lbpgqgkQ"
   },
   "source": [
    "## Comparison of BoW and BERT models\n",
    "\n",
    "The best `BoW` classifier and the `BERT` classifier are compared based on various criteria such as explainability or kinds of prediction errors they make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NS7aHnW9MGzW"
   },
   "source": [
    "#### Explainability\n",
    "\n",
    "`bert_classifier` does not have an equivalent to the `show_top_features` function to indicate predictive word-tokens for either ham or spam category. However, the `get_last_hidden_state` function outputs feature word-token `hidden states` which should have `cosine similarity` close to 1 for words that are used together within a particular context - for example, a ham or a spam message.\n",
    "\n",
    "In the below example, applying `sklearn.metrics.pairwise.cosine_similarity` on `linear_svm_classifier`'s top ham and spam feature `hidden states` should yield a value *far* from 1, while the top ham or top spam features should have `cosine similarity` *close* to 1. Of course, the quality of the results depends on how well `linear_svm_classifier` and `bert_classifier` have learned from the data and the data itself.\n",
    "\n",
    "How can the above technique support the purpose of explainability in `BERT`? Even without clear indication of spammy words, messages can - for example - be clustered by `cosine similarity`, with similar values indicating the same category (ham or spam). The latter can help with predictions or label spreading on new message data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rluHH9qIR2-K"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity_demo(\n",
    "  bert_classifier: BertForSequenceClassification,\n",
    "  bert_tokenizer: BertTokenizer,\n",
    "  bow_classifier_top_features: HamSpamFeatureImportances,\n",
    ") -> None:\n",
    "  top_ham_features = [name.split(\"TF-IDF__\")[1]  # N-grams (n > 1) skipped.\n",
    "                      for name, _ in bow_classifier_top_features[0]\n",
    "                      if name.startswith(\"TF-IDF__\")\n",
    "                         and all(not ch.isspace() for ch in name)]\n",
    "  top_ham_feature_reprs = [get_last_hidden_state(\n",
    "                             bert_classifier,\n",
    "                             bert_tokenizer,\n",
    "                             feature,\n",
    "                           )\n",
    "                           for feature in top_ham_features]\n",
    "\n",
    "  top_spam_features = [name.split(\"TF-IDF__\")[1]  # N-grams (n > 1) skipped.\n",
    "                       for name, _ in bow_classifier_top_features[1]\n",
    "                       if name.startswith(\"TF-IDF__\")\n",
    "                          and all(not ch.isspace() for ch in name)]\n",
    "  top_spam_feature_reprs = [get_last_hidden_state(\n",
    "                              bert_classifier,\n",
    "                              bert_tokenizer,\n",
    "                              feature,\n",
    "                            )\n",
    "                            for feature in top_spam_features]\n",
    "\n",
    "  ham_feature_data = list(zip(top_ham_features, top_ham_feature_reprs,\n",
    "                              [\"ham\"] * len(top_ham_features)))\n",
    "  spam_feature_data = list(zip(top_spam_features, top_spam_feature_reprs,\n",
    "                               [\"spam\"] * len(top_spam_features)))\n",
    "  feature_data = ham_feature_data + spam_feature_data\n",
    "\n",
    "  output = None\n",
    "  for f_data_a, f_data_b in itertools.combinations(feature_data, r=2):\n",
    "    f_a, f_repr_a, f_tag_a = f_data_a\n",
    "    f_b, f_repr_b, f_tag_b = f_data_b\n",
    "    try:\n",
    "      similarity = cosine_similarity(f_repr_a, f_repr_b)\n",
    "      output = f\"{similarity.ravel()[0]:.3f}\"\n",
    "    except ValueError as e:\n",
    "      output = f\"'{e}'\"\n",
    "    print(f\"cosine_similarity( \"\n",
    "            f\"{f_tag_a}({f_a}), \"\n",
    "            f\"{f_tag_b}({f_b}) ) => {output}\")\n",
    "\n",
    "\n",
    "def get_last_hidden_state(\n",
    "  bert_classifier: BertForSequenceClassification,\n",
    "  bert_tokenizer: BertTokenizer,\n",
    "  feature: str,\n",
    ") -> np.ndarray:\n",
    "  inputs = bert_tokenizer(feature,\n",
    "                          padding=\"max_length\",\n",
    "                          truncation=True,\n",
    "                          return_tensors=\"pt\")\n",
    "  with torch.no_grad():\n",
    "    last_hidden_state = bert_classifier(\n",
    "      **inputs,\n",
    "      output_hidden_states=True\n",
    "    ).hidden_states[-1].numpy().reshape(1, -1)\n",
    "  return last_hidden_state\n",
    "\n",
    "\n",
    "cosine_similarity_demo(bert_classifier, bert_tokenizer,\n",
    "                       linear_svm_top_features)  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5zlQABSqgkQ"
   },
   "source": [
    "### Prediction errors\n",
    "\n",
    "[RNN](#Recurrent-neural-network) models have the [vanishing gradients](#Vanishing-gradients) problem, which is also applicable for [BERT](#BERT)'s and every other deep model's architecture. So deep models have problems with processing long texts (truncation for big-sized text or padding for small-sized text). Processing of long texts is generally not an issue with `BoW` models (here no accounting for high dimensionality issues).\n",
    "\n",
    "In the below example is demonstrated how `bert_classifier` fails on label predictions and is outperformed by `best_bow_classifier` on many test set messages - mostly ones that take up roughly longer than `bert_preprocessor.tokenizer.model_max_length`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5_EvOKVqgkQ"
   },
   "outputs": [],
   "source": [
    "def prediction_errors_demo(\n",
    "  X_bow: pd.Series = X_test,\n",
    "  X_bert: datasets.Dataset = X_bert_test,\n",
    "  y: pd.Series = y_test,\n",
    ") -> None:\n",
    "  bow_predictions = best_bow_classifier.predict(X_bow)\n",
    "  bert_prob_predictions, bert_predictions = get_bert_predictions(\n",
    "    bert_classifier, bert_tokenizer, X_bert\n",
    "  )\n",
    "  bow_scored, bert_scored = 0, 0\n",
    "  bow_scored_ids, bert_scored_ids = [], []\n",
    "  bow_scored_sizes, bert_scored_sizes = [], []\n",
    "  for i, (bow_pred, bert_pred) in enumerate(zip(bow_predictions,\n",
    "                                                bert_predictions)):\n",
    "    message, is_spam = X_bow.iloc[i], y.iloc[i]\n",
    "    if bow_pred != bert_pred:\n",
    "      size = len(message.lower().split())\n",
    "      if is_spam == bow_pred:\n",
    "        bow_scored += 1\n",
    "        bow_scored_ids += [i]\n",
    "        bow_scored_sizes += [size]\n",
    "      else:\n",
    "        bert_scored += 1\n",
    "        bert_scored_ids += [i]\n",
    "        bert_scored_sizes += [size]\n",
    "\n",
    "  plt.scatter(bow_scored_ids, bow_scored_sizes,\n",
    "              label=f\"BoW({bow_scored})\")\n",
    "  plt.scatter(bert_scored_ids, bert_scored_sizes,\n",
    "              label=f\"BERT({bert_scored})\")\n",
    "  plt.xlabel(\"Message ID\")\n",
    "  plt.ylabel(\"Message size\")\n",
    "  plt.title(f\"Correctly predicted test message labels: BoW vs. BERT\")\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "prediction_errors_demo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDwZ2-rKdMb2"
   },
   "source": [
    "## Further development and testing\n",
    "\n",
    "Based on the `BoW`-`BERT` comparative analysis so far, it is evident that `BoW` text classification models pose less concerns as to their applicability for ham/spam classification. These `BoW` models inherently support very long texts and have clear explainability means such as the granularity of the features and the feature importances.\n",
    "\n",
    "Nevertheless, `BERT` models have their own unique strengths related to the context-based interpretation, which enables usage of similarity measures such as the `cosine similarity`, and also provides for better prediction `accuracy` by \"naturally\" covering the complex language use cases.\n",
    "\n",
    "Overall, and also evident in the examples part of this work, `BoW` and `BERT` models complement each other in many ways, which brings into consideration the possibility to apply some *voting* scheme to leverage predictions or generated data from both kinds of classifiers.\n",
    "\n",
    "Below is demonstrated how `best_bow_classifier` and `bert_classifier` can be tested with custom messages - inline in this notebook and also via user interface (UI) with [Gradio](#Gradio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom message test - inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BypzD-vwphFG"
   },
   "outputs": [],
   "source": [
    "custom_message_data = [\n",
    "  (\"this is a test\", 0),\n",
    "  (\"top offer\", 1),\n",
    "]\n",
    "custom_messages, custom_labels = zip(*custom_message_data)\n",
    "print(\"best_bow_classifier.score = \"\n",
    "      f\"{best_bow_classifier.score(custom_messages, custom_labels)}\")\n",
    "bert_custom_prob_predictions, bert_custom_predictions = get_bert_predictions(\n",
    "  bert_classifier, bert_tokenizer,\n",
    "  datasets.Dataset.from_dict({\n",
    "    \"message\": custom_messages\n",
    "  })\n",
    ")\n",
    "bert_custom_score = (\n",
    "  np.sum(bert_custom_predictions.numpy()\n",
    "         == np.array(custom_labels)).astype(np.int32)\n",
    "  / len(bert_custom_predictions)\n",
    ")\n",
    "print(f\"bert_classifier.score = {bert_custom_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom message test - UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ui.Blocks() as demo:\n",
    "  with ui.Row():\n",
    "    txt_message = ui.Textbox(label=\"message\")\n",
    "    txt_spam_proba = ui.Textbox(label=\"spam probability\")\n",
    "\n",
    "  with ui.Row():\n",
    "    btn_check = ui.Button(\"check\")\n",
    "    if hasattr(best_bow_classifier, \"predict_proba\"):\n",
    "      btn_check.click(\n",
    "        fn=lambda m: best_bow_classifier.predict_proba([m]).ravel()[1].round(3),\n",
    "        inputs=txt_message,\n",
    "        outputs=txt_spam_proba,\n",
    "      )\n",
    "    else:\n",
    "      btn_check.click(\n",
    "        fn=lambda m: best_bow_classifier.predict([m]).ravel()[0].round(3),\n",
    "        inputs=txt_message,\n",
    "        outputs=txt_spam_proba,\n",
    "      )\n",
    "  demo.launch(share=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### APA style for references\n",
    "American Psychological Association. (2022). Creating an APA Style reference list guide. https://apastyle.apa.org/instructional-aids/creating-reference-list.pdf\n",
    "\n",
    "American Psychological Association. (2024). APA Style common reference examples guide. https://apastyle.apa.org/instructional-aids/reference-examples.pdf\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Feature extraction methods\n",
    "<a name=\"TF-IDF\"></a>\n",
    "##### TF-IDF\n",
    "- [TFâ€“IDF - Wikipedia](https://en.wikipedia.org/wiki/Tf%E2%80%93idf)\n",
    "<a name=\"Feature-engineering\"></a>\n",
    "##### Feature engineering\n",
    "- [Feature engineering - Wikipedia](https://en.wikipedia.org/wiki/Feature_engineering)\n",
    "<a name=\"Word-embedding\"></a>\n",
    "##### Word embedding\n",
    "- [Word embedding - Wikipedia](https://en.wikipedia.org/wiki/Word_embedding)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning methods\n",
    "<a name=\"Naive-Bayes\"></a>\n",
    "##### Naive Bayes\n",
    "- [Naive Bayes classifier - Wikipedia](https://en.wikipedia.org/wiki/Naive_Bayes_classifier)\n",
    "<a name=\"Logistic-regression\"></a>\n",
    "##### Logistic regression\n",
    "- [Logistic regression - Wikipedia](https://en.wikipedia.org/wiki/Logistic_regression)\n",
    "<a name=\"Decision-tree\"></a>\n",
    "##### Decision tree\n",
    "- [Decision tree learning - Wikipedia](https://en.wikipedia.org/wiki/Decision_tree_learning)\n",
    "<a name=\"Support-vector-machine\"></a>\n",
    "##### Support vector machine\n",
    "- [Support vector machine - Wikipedia](https://en.wikipedia.org/wiki/Support_vector_machine)\n",
    "<a name=\"Ensemble-learning\"></a>\n",
    "##### Ensemble learning\n",
    "- [Ensemble learning - Wikipedia](https://en.wikipedia.org/wiki/Ensemble_learning)\n",
    "<a name=\"Recurrent-neural-network\"></a>\n",
    "##### Recurrent neural network\n",
    "- [Recurrent neural network - Wikipedia](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
    "<a name=\"Attention\"></a>\n",
    "##### Attention\n",
    "- [Attention (machine learning) - Wikipedia](https://en.wikipedia.org/wiki/Attention_(machine_learning))\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning models\n",
    "<a name=\"Bag-of-words\"></a>\n",
    "##### Bag-of-words\n",
    "- [Bag-of-words model - Wikipedia](https://en.wikipedia.org/wiki/Bag-of-words_model)\n",
    "<a name=\"BERT\"></a>\n",
    "##### BERT\n",
    "Turc, I., Chang, M. W., Lee, K., & Toutanova, K. (2019). Well-read students learn better: On the importance of pre-training compact models. *arXiv preprint arXiv:1908.08962v2*. https://doi.org/10.48550/arXiv.1908.08962\n",
    "- [BERT (language model) - Wikipedia](https://en.wikipedia.org/wiki/BERT_(language_model))\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Machine learning problems\n",
    "<a name=\"Vanishing-gradients\"></a>\n",
    "##### Vanishing gradients\n",
    "- [Vanishing gradient problem - Wikipedia](https://en.wikipedia.org/wiki/Vanishing_gradient_problem)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Datasets\n",
    "<a name=\"SpamAssassin-public-mail-corpus\"></a>\n",
    "##### SpamAssassin public mail corpus\n",
    "- [Index of /old/publiccorpus](https://spamassassin.apache.org/old/publiccorpus/)\n",
    "<a name=\"SMS-Spam-Collection---UCI-Machine-Learning-Repository\"></a>\n",
    "##### SMS Spam Collection - UCI Machine Learning Repository\n",
    "Almeida, T. & Hidalgo, J. (2011). SMS Spam Collection [Dataset]. *UCI Machine Learning Repository*. https://doi.org/10.24432/C5CC84.\n",
    "- [SMS Spam Collection - UCI Machine Learning Repository](https://archive.ics.uci.edu/dataset/228/)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Guides and tutorials\n",
    "- [Classification of text documents using sparse features â€” scikit-learn documentation](https://scikit-learn.org/stable/auto_examples/text/plot_document_classification_20newsgroups.html#analysis-of-a-bag-of-words-document-classifier)\n",
    "- [Text classification with an RNN Â |Â  TensorFlow](https://www.tensorflow.org/text/tutorials/text_classification_rnn)\n",
    "- [Classify text with BERT Â |Â  Text Â |Â  TensorFlow](https://www.tensorflow.org/text/tutorials/classify_text_with_bert)\n",
    "- [google/bert_uncased_L-2_H-128_A-2 Â· Hugging Face](https://huggingface.co/google/bert_uncased_L-2_H-128_A-2)\n",
    "- [Text classification Â· Hugging Face](https://huggingface.co/docs/transformers/tasks/sequence_classification)\n",
    "- [Trainer Â· Hugging Face](https://huggingface.co/docs/transformers/trainer)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Libraries\n",
    "<a name=\"Gradio\"></a>\n",
    "##### Gradio\n",
    "Abid, A., Abdalla, A., Abid, A., Khan, D., Alfozan, A., & Zou, J. (2019). Gradio: Hassle-free sharing and testing of ML models in the wild [Computer software]. https://doi.org/10.48550/arXiv.1906.02569\n",
    "- [Quickstart](https://www.gradio.app/guides/quickstart)\n",
    "<a name=\"Hugging-Face-Transformers\"></a>\n",
    "##### Hugging Face Transformers\n",
    "Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., & Rush, A. M. (2020). Transformers: State-of-the-Art Natural Language Processing [Conference paper]. 38â€“45. https://www.aclweb.org/anthology/2020.emnlp-demos.6\n",
    "- [Transformers](https://huggingface.co/docs/transformers/index)\n",
    "<a name=\"Keras\"></a>\n",
    "##### Keras\n",
    "Chollet, F., & others. (2015). Keras. https://keras.io\n",
    "- [Getting started with Keras](https://keras.io/getting_started/#tensorflow--keras-2-backwards-compatibility)\n",
    "##### Matplotlib\n",
    "Hunter, J. D. (May-June 2007). Matplotlib: A 2D Graphics Environment. *Computing in Science & Engineering*, *9*(3), 90-95. https://doi.org/10.1109/MCSE.2007.55\n",
    "- [Quick start guide](https://matplotlib.org/stable/users/explain/quick_start.html)\n",
    "##### NLTK\n",
    "Bird, S., Klein, E., & Loper, E. (2009). Natural Language Processing with Python: Analyzing Text with the Natural Language Toolkit. *O'Reilly Media, Inc.*. https://www.nltk.org/book/\n",
    "- [NLTK :: Natural Language Toolkit](https://www.nltk.org/)\n",
    "##### Numpy\n",
    "Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del RÃ­o, J. F., Wiebe, M., Peterson, P., ... Oliphant, T. E. (2020). Array programming with NumPy. *Nature*, *585*, 357â€“362. https://doi.org/10.1038/s41586-020-2649-2\n",
    "- [What is NumPy?](https://numpy.org/doc/2.2/user/whatisnumpy.html)\n",
    "##### Pandas\n",
    "The pandas development team. pandas-dev/pandas: Pandas [Computer software]. https://doi.org/10.5281/zenodo.3509134\n",
    "- [Getting started â€” pandas](https://pandas.pydata.org/docs/getting_started/index.html#intro-to-pandas)\n",
    "<a name=\"PyTorch\"></a>\n",
    "##### PyTorch\n",
    "Ansel, J., Yang, E., He, H., Gimelshein, N., Jain, A., Voznesensky, M., Bao, B., Bell, P., Berard, D., Burovski, E., Chauhan, G., Chourdia, A., Constable, W., Desmaison, A., DeVito, Z., Ellison, E., Feng, W., Gong, J., Gschwind, M., Hirsh, B., Huang, S., Kalambarkar, K., Kirsch, L., Lazos, M., Lezcano, M., Liang, Y., Liang, J., Lu, Y., Luk, C., Maher, B., Pan, Y., Puhrsch, C., Reso, M., Saroufim, M., Siraichi, M. Y., Suk, H., Suo, M., Tillet, P., Wang, E., Wang, X., Wen, W., Zhang, S., Zhao, X., Zhou, K., Zou, R., Mathews, A., Chanan, G., Wu, P., & Chintala, S. (2024). PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation [Conference paper]. 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2 (ASPLOS '24). https://doi.org/10.1145/3620665.3640366\n",
    "- [Start Locally | PyTorch](https://pytorch.org/get-started/locally)\n",
    "<a name=\"Scikit-learn\"></a>\n",
    "##### Scikit-learn\n",
    "Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M., Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D., Brucher, M., Perrot, M., & Duchesnay, Ã‰. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research*, *12*, 2825â€“2830. https://jmlr.csail.mit.edu/papers/v12/pedregosa11a.html\n",
    "- [Getting Started â€” scikit-learn](https://scikit-learn.org/stable/getting_started.html)\n",
    "<a name=\"TensorFlow\"></a>\n",
    "##### TensorFlow\n",
    "Abadi, M., Agarwal, A., Barham, P., Brevdo, E., Chen, Z., Citro, C., Corrado, G. S., Davis, A., Dean, J., Devin, M., Ghemawat, S., Goodfellow, I., Harp, A., Irving, G., Isard, M., Jozefowicz, R., Jia, Y., Kaiser, L., Kudlur, M., ... Zheng, X. (2015). TensorFlow, Large-scale machine learning on heterogeneous systems [Computer software]. https://doi.org/10.5281/zenodo.4724125\n",
    "- [Introduction to TensorFlow](https://www.tensorflow.org/learn)\n",
    "\n",
    "<br><br>\n",
    "\n",
    "### Tools\n",
    "<a name=\"Luigi\"></a>\n",
    "##### Luigi\n",
    "[Getting Started â€” Luigi](https://luigi.readthedocs.io/en/stable/)\n",
    "<a name=\"Optuna\"></a>\n",
    "##### Optuna\n",
    "Akiba, T., Sano, S., Yanase, T., Ohta, T., & Koyama, M. (2019). Optuna: A next-generation hyperparameter optimization framework [Conference paper]. *Proceedings of the 25th ACM SIGKDD international conference on knowledge discovery & data mining*, 2623â€“2631. https://doi.org/10.1145/3292500.3330701\n",
    "- [Optuna: A hyperparameter optimization framework](https://optuna.readthedocs.io/en/stable/)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
